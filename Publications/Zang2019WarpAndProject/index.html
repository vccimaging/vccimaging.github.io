---
layout: default
---
<div class="projectPage">
    
<h2> Warp-and-Project Tomography for Rapidly Deforming Objects </h2>

<a href="/People/zangg/">Guangming Zang</a>, <a href="/People/idoughr/">Ramzi Idoughi</a>, <a href="https://cohmas.kaust.edu.sa/Pages/Ran%20Tao.aspx">Ran Tao</a>, <a href="https://cohmas.kaust.edu.sa/Pages/Gilles%20Lubineau.aspx">Gilles Lubineau</a>, <a href="http://peterwonka.net/">Peter Wonka</a>, <a href="/People/heidriw/">Wolfgang Heidrich</a>

<br> Accepted to ACM Transactions on Graphics (Proc. SIGGRAPH), 2019 <br>

<hr size="4", color="black", width="100%">
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#results">Main results</a></li>
<li><a href="#paper">Paper, code and video</a></li>
</ul>
<hr size="4", color="black", width="100%">
    
<img src="teaser.png" width="100%"/> <br>
We introduce a CT reconstruction method for objects that undergo rapid deformation during the scan. 
Shown here is a copper foam crumpling under a compressive force during the scan. The whole complex 
animation is reconstructed using only 192 projection images that all correspond to different deformation
states of the foam. <br><br>
    
<h3 id="abstract">Abstract</h3>
Computed tomography has emerged as the method of choice for scanning
complex shapes as well as interior structures of stationary objects. Recent
progress has also allowed the use of CT for analyzing deforming objects and
dynamic phenomena, although the deformations have been constrained to
be either slow or periodic motions.<br>
In this work we improve the tomographic reconstruction of time-varying
geometries undergoing faster, non-periodic deformations. Our method uses
a warp-and-project approach that allows us to introduce an essentially
continuous time axis where consistency of the reconstructed shape with the
projection images is enforced for the specific time and deformation state at
which the image was captured. The method uses an efficient, time-adaptive
solver that yields both the moving geometry as well as the deformation field.<br>
We validate our method with extensive experiments using both synthetic
and real data from a range of different application scenarios.<br><br>
    
<h3 id="results">Main results</h3>



<h3 id="paper">Paper, code and video</h3>

    
</div>
