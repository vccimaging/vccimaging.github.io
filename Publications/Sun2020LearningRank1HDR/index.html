
<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

	.rotate {
		/* FF3.5+ */
		-moz-transform: rotate(-90.0deg);
		/* Opera 10.5 */
		-o-transform: rotate(-90.0deg);
		/* Saf3.1+, Chrome */
		-webkit-transform: rotate(-90.0deg);
		/* IE6,IE7 */
		filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
		/* IE8 */
		-ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
		/* Standard */
		transform: rotate(-90.0deg);
	}

	c {
	   white-space: nowrap;
       writing-mode: tb-rl;
       transform: rotate(-180.0deg);
	}

</style>

<html>


  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/vnd.microsoft.icon" id="favicon" />
    
    <!-- link to main stylesheet -->
    <link rel="stylesheet" type="text/css" href="/css/main.css">
  </head>
  
  <body>
    
    <img align=right src="./imgs/kaust.png" width=150 height=150>
    
    
    <nav>
      <ul>
        <li><a href="/">Home</a></li>
	<li><a href="/people.html">People</a></li>
	<li><a href="/publications.html">Publications</a></li>
        <li><a href="/joinus.html">Join Us</a></li>
	<li style="float:right"><a href=https://www.kaust.edu.sa/en>KAUST</a></li>
	<li style="float:right"><a href=https://cemse.kaust.edu.sa/vcc>VCC</a></li>
      </ul>
    </nav>
    
    <content>
      <div class="projectPage">
    
<script>
 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-98293807-1', 'auto');
 ga('send', 'pageview');

</script>





  <head>
        <title>Learning Rank-1 Diffractive Optics for Single-shot High Dynamic Range Imaging</title>
        <meta property="og:title" content="hdr" />
  </head>

  <body>
    <br>
    <center>
      <span style="font-size:36px">Learning Rank-1 Diffractive Optics for<br>Single-shot High Dynamic Range Imaging</span>
    </center>

    <br>
    <table align=center width=1098px>
      <tr>
        <td align=center width=90px>
        <center>
          <span style="font-size:20px"><a href="https://vccimaging.org/People/sunq/">Qilin Sun</a></span>
        </center>
        </td>

        <td align=center width=90px>
        <center>
          <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=6AB33cgAAAAJ&hl=en">Ethan Tseng</a></span>
        </center>
        </td>

        <td align=center width=90px>
        <center>
          <span style="font-size:20px"><a href="https://vccimaging.org/People/fuq/">Qiang Fu</a></span>
        </center>
        </td>

        <td align=center width=90px>
        <center>
          <span style="font-size:20px"><a href="https://vccimaging.org/People/heidriw/">Wolfgang Heidrich</a></span>
        </center>
        </td>

        <td align=center width=90px>
        <center>
          <span style="font-size:20px"><a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a></span>
        </center>
        </td>
      </tr>
    </table>

	  <br>
    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px">CVPR 2020 (Oral)</span>
        </center>
        </td>
      </tr>
    </table>

    <br>
    <br>
    <table align=center width=1098px>
      <tr>
        <td width=1098px>
          <center>
          <a href="./imgs/teaser.png"><img src = "./imgs/teaser.jpg" width="1098px"></img></a><br>
          </center>
        </td>
      </tr>
      </table>
      <table align=center width=900px></table>
        <tr>
          <td width=600px>
          <center>
            <span style="font-size:14px"><hr style="height:6pt; visibility:hidden;" /><i>We present a single-shot real-time HDR imaging technique. High dynamic range information is spatially encoded in a single image using a learned optical element, creating streak-like encodings, and a specialized reconstruction network to recover the HDR image from such measurements.</i>
          </center>
          </td>
        </tr>
        <tr>
          <td width=600px>
            <br>
            <p align="justify" style="font-size: 16px; line-height: 20px;">
High-dynamic range (HDR) imaging is an essential imaging modality for a wide range of applications in uncontrolled environments, including autonomous driving, robotics, and mobile phone cameras. However, existing HDR techniques in commodity devices struggle with dynamic scenes due to multi-shot acquisition and post-processing time, e.g. mobile phone burst photography, making such approaches unsuitable for real-time applications. In this work, we propose a method for snapshot HDR imaging by learning an optical HDR encoding in a single image which maps saturated highlights into neighboring unsaturated areas using a diffractive optical element (DOE). We propose a novel rank-1 parameterization of the DOE which drastically reduces the optical search space while allowing us to efficiently encode high-frequency detail. We propose a reconstruction network tailored to this rank-1 parametrization for the recovery of clipped information from the encoded measurements. The proposed end-to-end framework is validated through simulation and real-world experiments and improves the PSNR by more than 7 dB over state-of-the-art end-to-end designs.
            </p>
          </td>
        </tr>
      </table>

      <br>
      <hr>
      <!-- <table align=center width=550px> -->
      <table align=center width=720>
        <center><h1 style="margin-bottom:-5">Paper</h1></center>
        <tr>
          <td style="padding-left:0px;padding-right:20px;padding-top:0px;"><a href="./Sun2020LearningRank1HDR.pdf"><img style="height:180px; border: solid; border-radius:30px;" src="./LearnedOpticHDRThumbnail.jpg"/></a></td>
          <td><span style="font-size:16px">Qilin Sun, Ethan Tseng, Qiang Fu, Wolfgang Heidrich, Felix Heide
          <br>
          <br>
          Learning Rank-1 Diffractive Optics for Single-shot High Dynamic Range Imaging
          <br>
          <br>
          CVPR 2020 (Oral)</span>
          <br>
          </td>
        </tr>
      </table>
      <!--  <br> -->

      <table align=center width=720px style="padding-top: 14px;">
      <tr>
			<td align=center width=90px>
      <span style="font-size:18px"><center>
        <a href="./Sun2020LearningRank1HDR.pdf">[Paper]</a>
      </center></td>

			<td align=center width=90px>
      <span style="font-size:18px"><center>
        <a href="./Sun2020LearningRank1HDR_supp.pdf">[Supplement]</a>
      </center></td>

      <td align=center width=90px>
      <span style="font-size:18px"><center>
        <a href="./imgs/bib.txt">[Bibtex]</a>
      </center></td>

      <!--<td align=center width=90px>
      <span style="font-size:18px"><center>
          <a href="https://youtube.com">[Video]</a> 
      </center></td> -->

      <td align=center width=90px>
      <span style="font-size:18px"><center>
        <!-- <a href="https://github.com">[Code]</a> -->
        [Code]
      </center></td>
      </tr>
			</table>

			<!-- <table align=center width=700px style="padding-top: 5px;">
			<tr>
      <td colspan="5" style="font-size: 14px">
      <center>
        <i> Please address correspondence to <a href="mailto:qilin.sun@kaust.edu.sa">Qilin Sun</a>, <a href="mailto:eftseng@cs.princeton.edu">Ethan Tseng</a>, and <a href="mailto:fheide@cs.princeton.edu">Felix Heide</a>.</i>
      </center>
      </td>
      </tr>
      </table> --!>


		<br>
    <hr>
    <center><h1>Video Summary</h1></center>
    <br>
      <table align=center width=1098px>
        <tr>
            <td width=220px>
              <center>
                  <a href="./imgs/camera_prototype_high_res.png"><img src = "./imgs/camera_prototype.png" width="280px"></img></href></a><br>
                  <br>
                  <a href="./imgs/doe_high_res.jpg"><img src = "./imgs/doe.png" width="280px"></img></href></a><br>
            </center>
            </td>
            <td width=750px>
                <center>
                <video width="750" controls autoplay loop muted><source src="./imgs/video_summary.mp4#t=0" type="video/mp4"></video>
                </center>
            </td>
        </tr>
      </table>

      <table align=center width=900px></table>
        <tr>
          <td width=600px>
          <center>
            <span style="font-size:14px"><hr style="height:12pt; visibility:hidden;" /><i>Our camera prototype (top left) using our manufactured diffractive optical element (bottom left) along with video summary (right).</i>
            <br>
          </center>
          </td>
        </tr>
      </table>



		<br>
    <hr>
    <center><h1>End-to-end Designing Framework</h1></center>

  <!-- Real world captures -->
    <!-- Figures -->
		<table align=center width=900px>
		<tr>
			<td>
        <center>
          <a href="./mainfigure.png"><img src = "./mainfigure.png" width=1098px"></img></a><br>
        </center>
      </td>

    <!-- Caption -->
	  <table align=center width=900px>
	  <tr>
    <td>
		  <span style="font-size:18px"><hr style="height:2pt; visibility:hidden;" /><b>Framework for end-to-end designing and stagelized reconstruction.</b></span>
		  <p  align="justify" ><span style="font-size:15px">
       Our end-to-end pipeline consists of the image formation model and CNN reconstruction. Our CNN is divided into several stages that focus on separating the encoding from the captured LDR image, recovering the highlights, and fusing the recovered unsaturated and saturated regions to produce the final HDR prediction. After fabrication our image formation model is replaced by real-world captures.
      </span></p>
	  </td>
	  </tr>
	  </table>
		<br>
		<br>
    <center><h1>Selected Results</h1></center>

  <!-- Real world captures -->
    <!-- Figures -->
		<table align=center width=900px>
		<tr>
			<td>
        <center>
          <a href="./imgs/Real_World/experimental_results.png"><img src = "./imgs/Real_World/experimental_results.jpg" width=1098px"></img></a><br>
        </center>
      </td>

    <!-- Caption -->
	  <table align=center width=900px>
	  <tr>
    <td>
		  <span style="font-size:18px"><hr style="height:2pt; visibility:hidden;" /><b>Single-shot HDR imaging using specialized diffractive optic.</b></span>
		  <p  align="justify" ><span style="font-size:15px">
        Our fabricated DOE and catered reconstruction network allows us to perform snapshot HDR imaging. This is shown in the above real-world captures using our prototype. The DOE first spreads energy from saturated highlights into neighboring unsaturated regions. These streak encodings are used by our reconstruction network to recover the clipped highlights. The network also simultaneously removes the encoding streaks from the captured image. Both the DOE and the network are optimized end-to-end using a fully differentiable image formation model.
      </span></p>
	  </td>
	  </tr>
	  </table>
		<br>
		<br>


  <!-- Motion capture -->
    <!-- Figures -->
    <table align=center width=900px>
    <tr>
      <center>
        <a href="./imgs/Real_World/dynamic.png"><img src = "./imgs/Real_World/dynamic.jpg" width="1098px"></img></a><br>
      </center>
    </tr>
    </table>

    <!-- Caption -->
		<table align=center width=900px>
		<tr>
		<td>
      <span style="font-size:18px">
		  <hr style="height:2pt; visibility:hidden;"/><b>Capturing dynamic HDR scenes.</b>
      </span>
	    <p  align="justify" ><span style="font-size:15px">
        Our technique allows us to image complex dynamic HDR scenes that would be difficult to capture using traditional burst capture methods. The scene on the left consists of light sources on a swinging pendulum. Our method allows us to capture the HDR scene while avoiding the motion artifacts introduced by burst capture. The scene on the right consists of a strobe light array with 50 Hz frequency. Our snapshot HDR imaging technique is capable of capturing the light array at a single moment.
      </span></p>
    </td>
    </tr>
    </table>
		<br>
		<br>




  <!-- Simulation -->
    <!-- Figure -->
		<table align=center width=960px>
    <tr>
    <td>
      <center>
        <a href="./imgs/Simulation/comparisons.png"><img src = "./imgs/Simulation/comparisons.jpg" width="1098px" ></img></a><br>
      </center>
    </td>
    </tr>
    </table>

    <!-- Caption -->
	  <table align=center width=900px>
	  <tr>
    <td>
		  <span style="font-size:18px"><hr style="height:2pt; visibility:hidden;" /><b>Outperforming competitor single-shot HDR methods.</b></span>
		  <p  align="justify" ><span style="font-size:15px">
        We benchmarked our method against state-of-the-art single shot HDR imaging techniques in simulation, and we outperformed them by over 7 dB PSNR. Example qualitative results are shown above. HDR-CNN severely underestimates the intensity of saturated regions. Glare-HDR often leaves artifacts and fails to estimate highlights correctly. The copied peak encodings introduced by Deep Optics for HDR recovery often overlap with the saturated areas and consequently are ineffective for highlight reconstruction. Our method produces accurate highlight reconstructions while also correctly removing encoding artifacts.
      </span></p>
	  </td>
	  </tr>
	  </table>
		<br>


  <!-- Automotive -->
    <!-- Figures -->
		<table align=center width=800px>

    <tr>
    <td>
    <center>
        <a href="./imgs/Automotive/auto_streaks.png"><img src = "./imgs/Automotive/auto_streaks.jpg" width="1098px"></img></a><br>
    </center>
    </td>

		</table>
		<table align=center width=900px>
		<tr>
		<td>
    <hr style="height:2pt; visibility:hidden;" /><span style="font-size:18px"><b>Application to automative windshield streaks.</b>
    </span>
    <br>
    <p  align="justify" >
		<span style="font-size:15px">
      Our network's ability to remove streak encodings can also be applied to other types of streaks introduced by grating-like optics. Front-facing automotive cameras suffer from glare induced by thin lines of dust and dirt remaining on the windshield after wiping, see the diagram on the left. These thin streaks of dust produce glare streaks that vary with the wiping pattern on a curved windshield. Removing these streaks can improve autonomous driving at night time. We trained our network to remove these types of streaks and we demonstrate successful removal.
    </span>
		</p>
		</td>
    </tr>
    </table>

<br>
 <hr>
<!--		  <table align=center width=900px>
			  <tr>
          <td colspan="5" style="font-size: 14px">
          <center>
              We thank <a href="https://richzhang.github.io/colorization/">Richard Zhang</a> for his website template.
          </center>
          </td>
        </tr>
			</table>
-->

</body>
</html>

