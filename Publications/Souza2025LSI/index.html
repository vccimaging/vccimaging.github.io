---
layout: default
---
<div class="projectPage">
     
<h3> Latent Space Imaging </h3>

<a href="/People/souzama/">Matheus Souza</a>, <a href="/People/ydzheng/">Yidan Zheng</a>, <a href="/People/kangk">Kaizhang Kang</a>, <a href="/People/ynmishra">Yogeshwar Nath Mishra</a>, <a href="/People/fuq/">Qiang Fu</a>, <a href="/People/heidriw/">Wolfgang Heidrich</a>

<br> CVPR, 2025.  <br>

<hr size="4", color="black", width="100%">
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#paper">Paper</a></li>
</ul>
<hr size="4", color="black", width="100%">

<img src="teaser.jpg" width="100%"> <br>

<p>
We propose an extremely-compressed imaging paradigm called Latent Space Imaging (LSI). The optical encoder (<b>O</b>) projects the real signal into a compressed set of measurements. A digital encoder (D) then maps this signal to the latent space (<b>L</b>) of a frozen generative model (G), enabling image reconstruction. The <b>L</b> can also be linearly projected (P) to perform downstream tasks directly—such as facial segmentation (P<sub>S</sub>), landmark detection (P<sub>L</sub>), and attribute classification (P<sub>A</sub>)—without requiring image reconstruction or a complex new model.
</p>

<h4 id="abstract">Abstract</h4>
Digital imaging systems have traditionally relied on brute-force measurement and processing of pixels arranged on regular grids. In contrast, the human visual system performs significant data reduction from the large number of photoreceptors to the optic nerve, effectively encoding visual information into a low-bandwidth latent space representation optimized for brain processing. Inspired by this, we propose a similar approach to advance artificial vision systems.

Latent Space Imaging introduces a new paradigm that combines optics and software to encode image information directly into the semantically rich latent space of a generative model. This approach substantially reduces bandwidth and memory demands during image capture and enables a range of downstream tasks focused on the latent space.

We validate this principle through an initial hardware prototype based on a single-pixel camera. By implementing an amplitude modulation scheme that encodes into the generative model's latent space, we achieve compression ratios ranging from 1:100 to 1:1000 during imaging, and up to 1:16384 for downstream applications. This approach leverages the model's intrinsic linear boundaries, demonstrating the potential of latent space imaging for highly efficient imaging hardware, adaptable future applications in high-speed imaging, and task-specific cameras with significantly reduced hardware complexity.

<h4 id="paper">Paper</h4>
<pre class="tab">Paper                  <a href="Souza2025LSI.pdf">[Souza2025LSI.pdf]</a> </pre>
<pre class="tab">Supplementary          <a href="Souza2025LSI_supp.pdf">[Souza2025LSI_supp.pdf]</a> </pre>
<pre class="tab">Code                   <a href="https://github.com/vccimaging/latent-imaging">[https://github.com/vccimaging/latent-imaging]</a></pre> <br>
    

</div>
